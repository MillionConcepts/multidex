{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from operator import and_, not_, or_\n",
    "import functools\n",
    "import inspect\n",
    "import colorsys\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.attrs import LEMMA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from freqer import *\n",
    "from palettes import *\n",
    "from qualmath import *\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe at some point explicitly train a lemmatizer on novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 2000000\n",
    "copperfield_text = open('copperfield.txt').read()\n",
    "copperfield = nlp(copperfield_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obelisk_text = open('obelisk_gate.txt', encoding='utf-8').read()\n",
    "obelisk = nlp(obelisk_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "northanger_text = open('./novels/northanger.txt').read()\n",
    "northanger = nlp(northanger_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kids_text = open('./novels/just_kids.txt').read()\n",
    "kids = nlp(kids_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloody_text = open('./novels/bloody_chamber.txt').read()\n",
    "bloody = nlp(bloody_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spells_text = open('./3e_spells.txt').read()\n",
    "spells = nlp(spells_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 2000000\n",
    "got_text = open('./novels/got.txt').read()\n",
    "got = nlp(got_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nightwood_text = open('./novels/nightwood.txt').read()\n",
    "nightwood = nlp(nightwood_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in obelisk[1:1000]: print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compand' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5979eac86896>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplay_palette\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maqua\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbump\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\python-scratch\\freqer\\palettes.py\u001b[0m in \u001b[0;36mdisplay_palette\u001b[1;34m(palette_function, granularity, bump)\u001b[0m\n\u001b[0;32m     47\u001b[0m                     \u001b[0mwheel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                     \u001b[0mpeak_heat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                     \u001b[0mpeak_wheel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m                 )\n\u001b[0;32m     51\u001b[0m             )\n",
      "\u001b[1;32mD:\\python-scratch\\freqer\\palettes.py\u001b[0m in \u001b[0;36mcolor_getter\u001b[1;34m(palette_function, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     }\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpalette_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mrelevant_color_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdisplay_palette\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpalette_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgranularity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbump\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python-scratch\\freqer\\palettes.py\u001b[0m in \u001b[0;36maqua\u001b[1;34m(heat, peak_heat, wheel, peak_wheel)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maqua\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpeak_heat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwheel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpeak_wheel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheat\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeak_heat\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mgreen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheat\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mpeak_heat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmakeup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mblue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwheel\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mpeak_wheel\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheat\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mpeak_heat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmakeup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compand' is not defined"
     ]
    }
   ],
   "source": [
    "display_palette(aqua,bump=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_tokens(got, token_type=\"all\", boring=False, pos_list=None)[5].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorlist=draw_freq_chart(\n",
    "    freq_chart_prop,\n",
    "    0,\n",
    "    'average_position',\n",
    "    'frequency',\n",
    "    palette_function = purple,\n",
    "    background = [1,1,1],\n",
    "    wheel = 'average_position',\n",
    "    bump = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.polar(trimmed_chart['frequency'],trimmed_chart['average_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lemmalist = lemmatize(grab_tokens(copperfield))\n",
    "%time freq_chart = prep_freq_chart(copperfield,lemmalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can make this more efficient \n",
    "# even if you want to position _within_ all lemmas\n",
    "# by having the lemmatizer only _position_ the lemmas of interest\n",
    "\n",
    "%time lemmalist_prop = lemmatize(grab_tokens(copperfield,pos_list=['PROPN']))\n",
    "%time freq_chart_prop = prep_freq_chart(copperfield,lemmalist_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_chart[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimwords = ['say','go','come','mr.']\n",
    "trim_chart = freq_chart.loc[np.logical_not(freq_chart['word'].isin(trimwords))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smash_chart = copy.deepcopy(freq_chart)\n",
    "smash_chart[\"frequency\"] = smash_chart[\"frequency\"]**0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumb_sim(word,comp=\"cat\"):\n",
    "    return nlp(word).similarity(nlp(comp))\n",
    "smash_chart[\"like_cat\"] = smash_chart[\"word\"].apply(dumb_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smash_chart[\"like_night\"] = smash_chart[\"word\"].apply(dumb_sim,comp=\"night\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = draw_freq_chart(\n",
    "    smash_chart,\n",
    "    2,\n",
    "    'like_night',\n",
    "    'like_cat',\n",
    "    palette_function = purple,\n",
    "    background = [0,0,0],\n",
    "    wheel = 'average_position',\n",
    "    heat = 'average_position',\n",
    "    bump = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sin(smash_chart[\"frequency\"]/np.max(smash_chart[\"frequency\"])*100*math.pi))\n",
    "plt.plot(smash_chart[\"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.vstack(chart)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = np.arange(0,1,0.01)\n",
    "np.meshgrid(axis,axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plt.plot([0,3],'o',color=[0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_palette(aqua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(horrible).shapea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this more meaningful \n",
    "# we need a more robust concept of statistical distance\n",
    "# and actually a metric that's meaningful in this space is part of what we're thinking of\n",
    "# more broadly\n",
    "\n",
    "def average_distance(text_positions,item_1,item_2,absolute=True):\n",
    "    distances = [\n",
    "        np.min(text_positions[item_1] - pos)\n",
    "        for pos in text_positions[item_2]\n",
    "    ]\n",
    "    if absolute:\n",
    "        return [np.abs(np.average(distances)),np.std(distances)]\n",
    "    return [(np.average(distances)),np.std(distances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'janis' in freq_chart['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_word_distance_frame(index_word,freq_chart,lemmalist,text_positions):\n",
    "    word_frame = freq_chart.reindex(columns=['word','frequency'])\n",
    "    distlist = [\n",
    "        average_distance(\n",
    "            text_positions,index_word,word\n",
    "        ) for word in word_frame['word']\n",
    "    ]\n",
    "    avg_dist = pd.Series([stat[0] for stat in distlist],name=\"avg_dist\")\n",
    "    std_dist = pd.Series([stat[1] for stat in distlist], name=\"std_dist\")\n",
    "    return pd.concat([\n",
    "        word_frame,avg_dist,std_dist],axis=1).sort_values(\n",
    "        by='frequency',ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_positions=position_lemmas(copperfield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time morning_frame = single_word_distance_frame('heart',freq_chart,lemmalist,text_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morning_frame.loc[morning_frame['word']=='happiness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_freq_chart(\n",
    "    morning_frame[10:],\n",
    "    50,\n",
    "    'avg_dist',\n",
    "    'frequency',\n",
    "    palette_function = purple,\n",
    "    background = [0,0,0],\n",
    "    wheel = 'avg_dist',\n",
    "    heat = 'frequency',\n",
    "    bump = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rimbaud_frame = single_word_distance_frame('rimbaud',freq_chart,lemmalist,text_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time robert_frame = single_word_distance_frame('robert',freq_chart,lemmalist,text_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robert_frame.sort_values(by=\"avg_dist\").loc[robert_frame['frequency'] > 5][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_chart[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_frame = rimbaud_frame.loc[rimbaud_frame['word'] != 'robert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_freq_chart(trim_frame,5,'avg_dist','std_dist',colorfunc=purple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_way_comp(text_positions,item_1,item_2,test_item):\n",
    "    distances_1 = [\n",
    "        np.min(text_positions[test_item] - pos)\n",
    "        for pos in text_positions[item_1]\n",
    "    ]\n",
    "    distances_2 = [\n",
    "        np.min(text_positions[test_item] - pos) \n",
    "        for pos in text_positions[item_2]\n",
    "    ]\n",
    "    return [\n",
    "        np.average(distances_1),\n",
    "        np.average(distances_2),\n",
    "        np.std(distances_1),\n",
    "        np.std(distances_2)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_two_word_distance_frame(\n",
    "    index_word_1,index_word_2,freq_chart,lemmalist,text_positions\n",
    "):\n",
    "    word_frame = freq_chart.reindex(columns=['word','frequency','average_position'])\n",
    "    distlist = [\n",
    "        two_way_comp(\n",
    "            text_positions,index_word_1,index_word_2,word\n",
    "        ) for word in word_frame['word']\n",
    "    ]\n",
    "    avg_dist_1 = pd.Series([stat[0] for stat in distlist],name=\"avg_dist_1\")\n",
    "    avg_dist_2 = pd.Series([stat[1] for stat in distlist],name=\"avg_dist_2\")\n",
    "    std_dist_1 = pd.Series([stat[2] for stat in distlist], name=\"std_dist_1\")\n",
    "    std_dist_2 = pd.Series([stat[3] for stat in distlist], name=\"std_dist_2\")\n",
    "    naive_3_way = pd.Series(np.sqrt(avg_dist_1**2 + avg_dist_2**2), name = \"naive_3_way\")\n",
    "    return pd.concat([\n",
    "        word_frame,avg_dist_1,avg_dist_2,std_dist_1,std_dist_2,naive_3_way\n",
    "    ],axis=1).sort_values(\n",
    "        by='naive_3_way',ascending=True\n",
    "        ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_two_word_distance_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robert_rimbaud_frame = naive_two_word_distance_frame(\n",
    "    'robert','rimbaud',freq_chart,lemmalist,text_positions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "robert_rimbaud_frame[\"r\"] = (\n",
    "    robert_rimbaud_frame[\"avg_dist_1\"] ** 2 + robert_rimbaud_frame[\"avg_dist_2\"] ** 2\n",
    ") ** 0.5\n",
    "robert_rimbaud_frame[\"theta\"] = (\n",
    "    robert_rimbaud_frame[\"avg_dist_1\"] / robert_rimbaud_frame[\"avg_dist_2\"])\n",
    "robert_rimbaud_frame[\"diff\"] = (\n",
    "    robert_rimbaud_frame[\"avg_dist_1\"] - robert_rimbaud_frame[\"avg_dist_2\"] \n",
    ")\n",
    "\n",
    "robert_rimbaud_frame[\"pos_norm\"] = (\n",
    "    (robert_rimbaud_frame[\"average_position\"] - len(kids))/len(kids)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robert_rimbaud_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_freq_chart(robert_rimbaud_frame,15,'diff','avg_dist_2',colorfunc=purple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chart comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose_compare(token_1,token_2):\n",
    "    return {\n",
    "        'similarity':token_1.similarity(token_2),\n",
    "        'word_1':token_1.text,\n",
    "        'word_2':token_2.text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlist = [\n",
    "    verbose_compare(random.choice(tokenlist),random.choice(tokenlist)) \n",
    "    for n in range(2000)\n",
    "]\n",
    "simlist = sorted(simlist, key=itemgetter('similarity'), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=nlp('dog')\n",
    "simlist = [\n",
    "    verbose_compare(word,token)\n",
    "    for token in tokenlist\n",
    "]\n",
    "simlist = sorted(simlist, key=itemgetter('similarity'), reverse=True)\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist([item['similarity'] for item in simlist], bins=50)\n",
    "\n",
    "word=nlp('cat')\n",
    "simlist = [\n",
    "    verbose_compare(word,token)\n",
    "    for token in tokenlist\n",
    "]\n",
    "simlist = sorted(simlist, key=itemgetter('similarity'), reverse=True)\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist([item['similarity'] for item in simlist], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so doing the whole novel like this would take 25-27 hours on this laptop. \n",
    "# much faster if we parallelized it...\n",
    "# maybe just the most frequent words?\n",
    "\n",
    "i = 0\n",
    "simdict = {}\n",
    "\n",
    "start = timeit.default_timer()\n",
    "run_size = len(tokenset)\n",
    "\n",
    "for token in tokenset:\n",
    "    i+=1\n",
    "    print(str(i)+'/'+str(run_size)+':'+token.text)\n",
    "    \n",
    "    simdict[token] = [\n",
    "        verbose_compare(token,word) for word in tokenset\n",
    "    ]\n",
    "    cur_time = (timeit.default_timer()-start)\n",
    "    \n",
    "    \n",
    "    print('current runtime: ', cur_time)\n",
    "    print('expected runtime: ', cur_time * (run_size/i))\n",
    "    \n",
    "    if i == 50:\n",
    "        break\n",
    "    \n",
    "\n",
    "# all_similar = [\n",
    "#     [verbose_compare(word,token) for token in tokenset] \n",
    "#     for word in tokenset\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a list of tokens -- or lemmas? by frequency.\n",
    "# use that to weight the similarity calculation.\n",
    "\n",
    "# we could also just go back to an idea about frequency / recency space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## section for exploring k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
