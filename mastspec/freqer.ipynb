{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from operator import and_, not_, or_\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from jupyter_plotly_dash import JupyterDash\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.attrs import LEMMA\n",
    "import en_core_web_lg\n",
    "\n",
    "from freqer import *\n",
    "from palettes import *\n",
    "from qualmath import *\n",
    "from miketools import *\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "# spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning: make sure to have lots of RAM\n",
    "# if you're going to feed it something large.\n",
    "\n",
    "textfile = 'jekyllhyde.txt'\n",
    "active_text = open(textfile).read()\n",
    "# if len(active_text) > 1000000:\n",
    "#     nlp.max_length = len(active_text) * 1.1\n",
    "active_text = nlp(active_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in active_text[1:20]: print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_palette(aqua,bump=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_tokens(active_text, token_type=\"all\", boring=False, pos_list=None)[5].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lemmalist = lemmatize(grab_tokens(active_text))\n",
    "%time freq_chart = prep_freq_chart(active_text,lemmalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lemmalist_prop = lemmatize(grab_tokens(active_text,pos_list=['PROPN']))\n",
    "%time freq_chart_prop = prep_freq_chart(active_text,lemmalist_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimwords = ['say','go','come','mr.','mrs.','miss']\n",
    "trim_chart = notdf(freq_chart,inloc(freq_chart,\"word\",trimwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smash_chart = copy.deepcopy(freq_chart)\n",
    "smash_chart[\"frequency\"] = smash_chart[\"frequency\"]**0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dumb_sim(word,comp=\"cat\"):\n",
    "    \"\"\"proper nouns, etc., do not possess vectors\"\"\"\n",
    "    return nlp(word).similarity(nlp(comp))\n",
    "smash_chart[\"like_cat\"] = smash_chart[\"word\"].apply(dumb_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smash_chart[\"like_night\"] = smash_chart[\"word\"].apply(dumb_sim,comp=\"night\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = draw_freq_chart(\n",
    "    trim_chart,\n",
    "    2,\n",
    "    'average_position',\n",
    "    'frequency',\n",
    "    palette_function = aqua,\n",
    "    background = [0,0,0],\n",
    "    wheel = 'stdev_position',\n",
    "    heat = 'frequency',\n",
    "    bump = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sin(smash_chart[\"frequency\"]/np.max(smash_chart[\"frequency\"])*100*math.pi))\n",
    "plt.plot(smash_chart[\"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.vstack(chart)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = np.arange(0,1,0.01)\n",
    "np.meshgrid(axis,axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_palette(aqua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this more meaningful \n",
    "# we need a more robust concept of statistical distance\n",
    "# and actually a metric that's meaningful in this space is part of what we're thinking of\n",
    "# more broadly\n",
    "\n",
    "def average_distance(text_positions,item_1,item_2,absolute=True):\n",
    "    distances = [\n",
    "        np.min(text_positions[item_1] - pos)\n",
    "        for pos in text_positions[item_2]\n",
    "    ]\n",
    "    if absolute:\n",
    "        return [np.abs(np.average(distances)),np.std(distances)]\n",
    "    return [(np.average(distances)),np.std(distances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'janis' in freq_chart['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_word_distance_frame(index_word,freq_chart,lemmalist,text_positions):\n",
    "    word_frame = freq_chart.reindex(columns=['word','frequency'])\n",
    "    distlist = [\n",
    "        average_distance(\n",
    "            text_positions,index_word,word\n",
    "        ) for word in word_frame['word']\n",
    "    ]\n",
    "    avg_dist = pd.Series([stat[0] for stat in distlist],name=\"avg_dist\")\n",
    "    std_dist = pd.Series([stat[1] for stat in distlist], name=\"std_dist\")\n",
    "    return pd.concat([\n",
    "        word_frame,avg_dist,std_dist],axis=1).sort_values(\n",
    "        by='frequency',ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_positions=position_lemmas(copperfield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time morning_frame = single_word_distance_frame('heart',freq_chart,lemmalist,text_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morning_frame.loc[morning_frame['word']=='happiness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_freq_chart(\n",
    "    morning_frame[10:],\n",
    "    50,\n",
    "    'avg_dist',\n",
    "    'frequency',\n",
    "    palette_function = purple,\n",
    "    background = [0,0,0],\n",
    "    wheel = 'avg_dist',\n",
    "    heat = 'frequency',\n",
    "    bump = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rimbaud_frame = single_word_distance_frame('rimbaud',freq_chart,lemmalist,text_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time robert_frame = single_word_distance_frame('robert',freq_chart,lemmalist,text_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robert_frame.sort_values(by=\"avg_dist\").loc[robert_frame['frequency'] > 5][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_chart[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_frame = rimbaud_frame.loc[rimbaud_frame['word'] != 'robert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_freq_chart(trim_frame,5,'avg_dist','std_dist',colorfunc=purple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_way_comp(text_positions,item_1,item_2,test_item):\n",
    "    distances_1 = [\n",
    "        np.min(text_positions[test_item] - pos)\n",
    "        for pos in text_positions[item_1]\n",
    "    ]\n",
    "    distances_2 = [\n",
    "        np.min(text_positions[test_item] - pos) \n",
    "        for pos in text_positions[item_2]\n",
    "    ]\n",
    "    return [\n",
    "        np.average(distances_1),\n",
    "        np.average(distances_2),\n",
    "        np.std(distances_1),\n",
    "        np.std(distances_2)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_two_word_distance_frame(\n",
    "    index_word_1,index_word_2,freq_chart,lemmalist,text_positions\n",
    "):\n",
    "    word_frame = freq_chart.reindex(columns=['word','frequency','average_position'])\n",
    "    distlist = [\n",
    "        two_way_comp(\n",
    "            text_positions,index_word_1,index_word_2,word\n",
    "        ) for word in word_frame['word']\n",
    "    ]\n",
    "    avg_dist_1 = pd.Series([stat[0] for stat in distlist],name=\"avg_dist_1\")\n",
    "    avg_dist_2 = pd.Series([stat[1] for stat in distlist],name=\"avg_dist_2\")\n",
    "    std_dist_1 = pd.Series([stat[2] for stat in distlist], name=\"std_dist_1\")\n",
    "    std_dist_2 = pd.Series([stat[3] for stat in distlist], name=\"std_dist_2\")\n",
    "    naive_3_way = pd.Series(np.sqrt(avg_dist_1**2 + avg_dist_2**2), name = \"naive_3_way\")\n",
    "    return pd.concat([\n",
    "        word_frame,avg_dist_1,avg_dist_2,std_dist_1,std_dist_2,naive_3_way\n",
    "    ],axis=1).sort_values(\n",
    "        by='naive_3_way',ascending=True\n",
    "        ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_two_word_distance_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robert_rimbaud_frame = naive_two_word_distance_frame(\n",
    "    'robert','rimbaud',freq_chart,lemmalist,text_positions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "robert_rimbaud_frame[\"r\"] = (\n",
    "    robert_rimbaud_frame[\"avg_dist_1\"] ** 2 + robert_rimbaud_frame[\"avg_dist_2\"] ** 2\n",
    ") ** 0.5\n",
    "robert_rimbaud_frame[\"theta\"] = (\n",
    "    robert_rimbaud_frame[\"avg_dist_1\"] / robert_rimbaud_frame[\"avg_dist_2\"])\n",
    "robert_rimbaud_frame[\"diff\"] = (\n",
    "    robert_rimbaud_frame[\"avg_dist_1\"] - robert_rimbaud_frame[\"avg_dist_2\"] \n",
    ")\n",
    "\n",
    "robert_rimbaud_frame[\"pos_norm\"] = (\n",
    "    (robert_rimbaud_frame[\"average_position\"] - len(kids))/len(kids)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robert_rimbaud_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_freq_chart(robert_rimbaud_frame,15,'diff','avg_dist_2',colorfunc=purple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chart comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose_compare(token_1,token_2):\n",
    "    return {\n",
    "        'similarity':token_1.similarity(token_2),\n",
    "        'word_1':token_1.text,\n",
    "        'word_2':token_2.text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlist = [\n",
    "    verbose_compare(random.choice(tokenlist),random.choice(tokenlist)) \n",
    "    for n in range(2000)\n",
    "]\n",
    "simlist = sorted(simlist, key=itemgetter('similarity'), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=nlp('dog')\n",
    "simlist = [\n",
    "    verbose_compare(word,token)\n",
    "    for token in tokenlist\n",
    "]\n",
    "simlist = sorted(simlist, key=itemgetter('similarity'), reverse=True)\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist([item['similarity'] for item in simlist], bins=50)\n",
    "\n",
    "word=nlp('cat')\n",
    "simlist = [\n",
    "    verbose_compare(word,token)\n",
    "    for token in tokenlist\n",
    "]\n",
    "simlist = sorted(simlist, key=itemgetter('similarity'), reverse=True)\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist([item['similarity'] for item in simlist], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so doing the whole novel like this would take 25-27 hours on this laptop. \n",
    "# much faster if we parallelized it...\n",
    "# maybe just the most frequent words?\n",
    "\n",
    "i = 0\n",
    "simdict = {}\n",
    "\n",
    "start = timeit.default_timer()\n",
    "run_size = len(tokenset)\n",
    "\n",
    "for token in tokenset:\n",
    "    i+=1\n",
    "    print(str(i)+'/'+str(run_size)+':'+token.text)\n",
    "    \n",
    "    simdict[token] = [\n",
    "        verbose_compare(token,word) for word in tokenset\n",
    "    ]\n",
    "    cur_time = (timeit.default_timer()-start)\n",
    "    \n",
    "    \n",
    "    print('current runtime: ', cur_time)\n",
    "    print('expected runtime: ', cur_time * (run_size/i))\n",
    "    \n",
    "    if i == 50:\n",
    "        break\n",
    "    \n",
    "\n",
    "# all_similar = [\n",
    "#     [verbose_compare(word,token) for token in tokenset] \n",
    "#     for word in tokenset\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a list of tokens -- or lemmas? by frequency.\n",
    "# use that to weight the similarity calculation.\n",
    "\n",
    "# we could also just go back to an idea about frequency / recency space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## section for exploring k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
